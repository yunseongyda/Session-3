{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 100, Loss: 4.345\n",
      "Epoch 1, Batch 200, Loss: 3.952\n",
      "Epoch 1, Batch 300, Loss: 3.789\n",
      "Epoch 1, Batch 400, Loss: 3.647\n",
      "Epoch 1, Batch 500, Loss: 3.533\n",
      "Epoch 1, Batch 600, Loss: 3.462\n",
      "Epoch 1, Batch 700, Loss: 3.300\n",
      "Epoch 2, Batch 100, Loss: 3.125\n",
      "Epoch 2, Batch 200, Loss: 3.121\n",
      "Epoch 2, Batch 300, Loss: 3.078\n",
      "Epoch 2, Batch 400, Loss: 3.038\n",
      "Epoch 2, Batch 500, Loss: 3.003\n",
      "Epoch 2, Batch 600, Loss: 2.939\n",
      "Epoch 2, Batch 700, Loss: 2.915\n",
      "Epoch 3, Batch 100, Loss: 2.790\n",
      "Epoch 3, Batch 200, Loss: 2.749\n",
      "Epoch 3, Batch 300, Loss: 2.805\n",
      "Epoch 3, Batch 400, Loss: 2.744\n",
      "Epoch 3, Batch 500, Loss: 2.733\n",
      "Epoch 3, Batch 600, Loss: 2.701\n",
      "Epoch 3, Batch 700, Loss: 2.678\n",
      "Epoch 4, Batch 100, Loss: 2.578\n",
      "Epoch 4, Batch 200, Loss: 2.597\n",
      "Epoch 4, Batch 300, Loss: 2.583\n",
      "Epoch 4, Batch 400, Loss: 2.535\n",
      "Epoch 4, Batch 500, Loss: 2.535\n",
      "Epoch 4, Batch 600, Loss: 2.485\n",
      "Epoch 4, Batch 700, Loss: 2.516\n",
      "Epoch 5, Batch 100, Loss: 2.412\n",
      "Epoch 5, Batch 200, Loss: 2.462\n",
      "Epoch 5, Batch 300, Loss: 2.411\n",
      "Epoch 5, Batch 400, Loss: 2.394\n",
      "Epoch 5, Batch 500, Loss: 2.380\n",
      "Epoch 5, Batch 600, Loss: 2.394\n",
      "Epoch 5, Batch 700, Loss: 2.388\n",
      "Epoch 6, Batch 100, Loss: 2.293\n",
      "Epoch 6, Batch 200, Loss: 2.305\n",
      "Epoch 6, Batch 300, Loss: 2.337\n",
      "Epoch 6, Batch 400, Loss: 2.296\n",
      "Epoch 6, Batch 500, Loss: 2.284\n",
      "Epoch 6, Batch 600, Loss: 2.319\n",
      "Epoch 6, Batch 700, Loss: 2.294\n",
      "Epoch 7, Batch 100, Loss: 2.220\n",
      "Epoch 7, Batch 200, Loss: 2.193\n",
      "Epoch 7, Batch 300, Loss: 2.188\n",
      "Epoch 7, Batch 400, Loss: 2.206\n",
      "Epoch 7, Batch 500, Loss: 2.189\n",
      "Epoch 7, Batch 600, Loss: 2.215\n",
      "Epoch 7, Batch 700, Loss: 2.221\n",
      "Epoch 8, Batch 100, Loss: 2.128\n",
      "Epoch 8, Batch 200, Loss: 2.147\n",
      "Epoch 8, Batch 300, Loss: 2.123\n",
      "Epoch 8, Batch 400, Loss: 2.140\n",
      "Epoch 8, Batch 500, Loss: 2.157\n",
      "Epoch 8, Batch 600, Loss: 2.128\n",
      "Epoch 8, Batch 700, Loss: 2.140\n",
      "Epoch 9, Batch 100, Loss: 2.047\n",
      "Epoch 9, Batch 200, Loss: 2.035\n",
      "Epoch 9, Batch 300, Loss: 2.068\n",
      "Epoch 9, Batch 400, Loss: 2.111\n",
      "Epoch 9, Batch 500, Loss: 2.080\n",
      "Epoch 9, Batch 600, Loss: 2.049\n",
      "Epoch 9, Batch 700, Loss: 2.075\n",
      "Epoch 10, Batch 100, Loss: 2.017\n",
      "Epoch 10, Batch 200, Loss: 1.984\n",
      "Epoch 10, Batch 300, Loss: 2.026\n",
      "Epoch 10, Batch 400, Loss: 2.025\n",
      "Epoch 10, Batch 500, Loss: 1.979\n",
      "Epoch 10, Batch 600, Loss: 2.015\n",
      "Epoch 10, Batch 700, Loss: 2.014\n",
      "Epoch 11, Batch 100, Loss: 1.883\n",
      "Epoch 11, Batch 200, Loss: 1.887\n",
      "Epoch 11, Batch 300, Loss: 1.853\n",
      "Epoch 11, Batch 400, Loss: 1.857\n",
      "Epoch 11, Batch 500, Loss: 1.852\n",
      "Epoch 11, Batch 600, Loss: 1.841\n",
      "Epoch 11, Batch 700, Loss: 1.898\n",
      "Epoch 12, Batch 100, Loss: 1.836\n",
      "Epoch 12, Batch 200, Loss: 1.785\n",
      "Epoch 12, Batch 300, Loss: 1.844\n",
      "Epoch 12, Batch 400, Loss: 1.826\n",
      "Epoch 12, Batch 500, Loss: 1.790\n",
      "Epoch 12, Batch 600, Loss: 1.833\n",
      "Epoch 12, Batch 700, Loss: 1.797\n",
      "Epoch 13, Batch 100, Loss: 1.775\n",
      "Epoch 13, Batch 200, Loss: 1.745\n",
      "Epoch 13, Batch 300, Loss: 1.780\n",
      "Epoch 13, Batch 400, Loss: 1.812\n",
      "Epoch 13, Batch 500, Loss: 1.796\n",
      "Epoch 13, Batch 600, Loss: 1.797\n",
      "Epoch 13, Batch 700, Loss: 1.788\n",
      "Epoch 14, Batch 100, Loss: 1.739\n",
      "Epoch 14, Batch 200, Loss: 1.743\n",
      "Epoch 14, Batch 300, Loss: 1.764\n",
      "Epoch 14, Batch 400, Loss: 1.783\n",
      "Epoch 14, Batch 500, Loss: 1.761\n",
      "Epoch 14, Batch 600, Loss: 1.764\n",
      "Epoch 14, Batch 700, Loss: 1.751\n",
      "Epoch 15, Batch 100, Loss: 1.712\n",
      "Epoch 15, Batch 200, Loss: 1.674\n",
      "Epoch 15, Batch 300, Loss: 1.748\n",
      "Epoch 15, Batch 400, Loss: 1.703\n",
      "Epoch 15, Batch 500, Loss: 1.717\n",
      "Epoch 15, Batch 600, Loss: 1.735\n",
      "Epoch 15, Batch 700, Loss: 1.764\n",
      "Epoch 16, Batch 100, Loss: 1.692\n",
      "Epoch 16, Batch 200, Loss: 1.723\n",
      "Epoch 16, Batch 300, Loss: 1.707\n",
      "Epoch 16, Batch 400, Loss: 1.690\n",
      "Epoch 16, Batch 500, Loss: 1.720\n",
      "Epoch 16, Batch 600, Loss: 1.726\n",
      "Epoch 16, Batch 700, Loss: 1.696\n",
      "Epoch 17, Batch 100, Loss: 1.710\n",
      "Epoch 17, Batch 200, Loss: 1.696\n",
      "Epoch 17, Batch 300, Loss: 1.679\n",
      "Epoch 17, Batch 400, Loss: 1.678\n",
      "Epoch 17, Batch 500, Loss: 1.654\n",
      "Epoch 17, Batch 600, Loss: 1.650\n",
      "Epoch 17, Batch 700, Loss: 1.720\n",
      "Epoch 18, Batch 100, Loss: 1.690\n",
      "Epoch 18, Batch 200, Loss: 1.622\n",
      "Epoch 18, Batch 300, Loss: 1.629\n",
      "Epoch 18, Batch 400, Loss: 1.679\n",
      "Epoch 18, Batch 500, Loss: 1.670\n",
      "Epoch 18, Batch 600, Loss: 1.666\n",
      "Epoch 18, Batch 700, Loss: 1.669\n",
      "Epoch 19, Batch 100, Loss: 1.646\n",
      "Epoch 19, Batch 200, Loss: 1.610\n",
      "Epoch 19, Batch 300, Loss: 1.645\n",
      "Epoch 19, Batch 400, Loss: 1.638\n",
      "Epoch 19, Batch 500, Loss: 1.665\n",
      "Epoch 19, Batch 600, Loss: 1.653\n",
      "Epoch 19, Batch 700, Loss: 1.634\n",
      "Epoch 20, Batch 100, Loss: 1.643\n",
      "Epoch 20, Batch 200, Loss: 1.600\n",
      "Epoch 20, Batch 300, Loss: 1.645\n",
      "Epoch 20, Batch 400, Loss: 1.627\n",
      "Epoch 20, Batch 500, Loss: 1.634\n",
      "Epoch 20, Batch 600, Loss: 1.604\n",
      "Epoch 20, Batch 700, Loss: 1.632\n",
      "Epoch 21, Batch 100, Loss: 1.535\n",
      "Epoch 21, Batch 200, Loss: 1.537\n",
      "Epoch 21, Batch 300, Loss: 1.561\n",
      "Epoch 21, Batch 400, Loss: 1.549\n",
      "Epoch 21, Batch 500, Loss: 1.513\n",
      "Epoch 21, Batch 600, Loss: 1.600\n",
      "Epoch 21, Batch 700, Loss: 1.544\n",
      "Epoch 22, Batch 100, Loss: 1.528\n",
      "Epoch 22, Batch 200, Loss: 1.501\n",
      "Epoch 22, Batch 300, Loss: 1.521\n",
      "Epoch 22, Batch 400, Loss: 1.501\n",
      "Epoch 22, Batch 500, Loss: 1.480\n",
      "Epoch 22, Batch 600, Loss: 1.538\n",
      "Epoch 22, Batch 700, Loss: 1.543\n",
      "Epoch 23, Batch 100, Loss: 1.546\n",
      "Epoch 23, Batch 200, Loss: 1.545\n",
      "Epoch 23, Batch 300, Loss: 1.494\n",
      "Epoch 23, Batch 400, Loss: 1.523\n",
      "Epoch 23, Batch 500, Loss: 1.509\n",
      "Epoch 23, Batch 600, Loss: 1.484\n",
      "Epoch 23, Batch 700, Loss: 1.513\n",
      "Epoch 24, Batch 100, Loss: 1.497\n",
      "Epoch 24, Batch 200, Loss: 1.491\n",
      "Epoch 24, Batch 300, Loss: 1.522\n",
      "Epoch 24, Batch 400, Loss: 1.466\n",
      "Epoch 24, Batch 500, Loss: 1.474\n",
      "Epoch 24, Batch 600, Loss: 1.484\n",
      "Epoch 24, Batch 700, Loss: 1.538\n",
      "Epoch 25, Batch 100, Loss: 1.486\n",
      "Epoch 25, Batch 200, Loss: 1.467\n",
      "Epoch 25, Batch 300, Loss: 1.503\n",
      "Epoch 25, Batch 400, Loss: 1.473\n",
      "Epoch 25, Batch 500, Loss: 1.487\n",
      "Epoch 25, Batch 600, Loss: 1.491\n",
      "Epoch 25, Batch 700, Loss: 1.506\n",
      "Epoch 26, Batch 100, Loss: 1.467\n",
      "Epoch 26, Batch 200, Loss: 1.507\n",
      "Epoch 26, Batch 300, Loss: 1.454\n",
      "Epoch 26, Batch 400, Loss: 1.471\n",
      "Epoch 26, Batch 500, Loss: 1.424\n",
      "Epoch 26, Batch 600, Loss: 1.517\n",
      "Epoch 26, Batch 700, Loss: 1.457\n",
      "Epoch 27, Batch 100, Loss: 1.468\n",
      "Epoch 27, Batch 200, Loss: 1.470\n",
      "Epoch 27, Batch 300, Loss: 1.474\n",
      "Epoch 27, Batch 400, Loss: 1.497\n",
      "Epoch 27, Batch 500, Loss: 1.484\n",
      "Epoch 27, Batch 600, Loss: 1.465\n",
      "Epoch 27, Batch 700, Loss: 1.449\n",
      "Epoch 28, Batch 100, Loss: 1.472\n",
      "Epoch 28, Batch 200, Loss: 1.472\n",
      "Epoch 28, Batch 300, Loss: 1.470\n",
      "Epoch 28, Batch 400, Loss: 1.423\n",
      "Epoch 28, Batch 500, Loss: 1.484\n",
      "Epoch 28, Batch 600, Loss: 1.477\n",
      "Epoch 28, Batch 700, Loss: 1.418\n",
      "Epoch 29, Batch 100, Loss: 1.413\n",
      "Epoch 29, Batch 200, Loss: 1.464\n",
      "Epoch 29, Batch 300, Loss: 1.450\n",
      "Epoch 29, Batch 400, Loss: 1.462\n",
      "Epoch 29, Batch 500, Loss: 1.431\n",
      "Epoch 29, Batch 600, Loss: 1.469\n",
      "Epoch 29, Batch 700, Loss: 1.450\n",
      "Epoch 30, Batch 100, Loss: 1.430\n",
      "Epoch 30, Batch 200, Loss: 1.416\n",
      "Epoch 30, Batch 300, Loss: 1.426\n",
      "Epoch 30, Batch 400, Loss: 1.440\n",
      "Epoch 30, Batch 500, Loss: 1.425\n",
      "Epoch 30, Batch 600, Loss: 1.416\n",
      "Epoch 30, Batch 700, Loss: 1.478\n",
      "Accuracy on test set: 59.14%\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리 및 데이터 증강\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),  # 무작위로 잘라서 일반화\n",
    "    transforms.RandomHorizontalFlip(),    # 랜덤 좌우반전\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  # 색상 변화\n",
    "    transforms.RandomAdjustSharpness(2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# 데이터셋 로드\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 개선된 CNN 모델 정의\n",
    "class ImprovedCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImprovedCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc1 = nn.Linear(256 * 4 * 4, 512)\n",
    "        self.fc2 = nn.Linear(512, 100)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))  # 32x32 -> 16x16\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))  # 16x16 -> 8x8\n",
    "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))  # 8x8 -> 4x4\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1, 256 * 4 * 4)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 모델 생성\n",
    "model = ImprovedCNN()\n",
    "\n",
    "# 손실 함수 및 최적화 기법\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# 훈련 함수\n",
    "def train(model, trainloader, criterion, optimizer, scheduler, num_epochs=30):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(trainloader, 0):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 100 == 99:\n",
    "                print(f\"Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100:.3f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "# 테스트 함수\n",
    "def test(model, testloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy on test set: {100 * correct / total:.2f}%')\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "train(model, trainloader, criterion, optimizer, scheduler, num_epochs=30)\n",
    "test(model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
